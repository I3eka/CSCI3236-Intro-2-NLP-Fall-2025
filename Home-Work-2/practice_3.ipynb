{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them',\n",
    "'So when they were old enough, she sent them out into the world to seek their fortunes.',\n",
    "'The first little pig was very lazy',\n",
    "\"He didn't want to work at all and he built his house out of straw\",\n",
    "'The second little pig worked a little bit harder but he was somewhat lazy too and he built his house out of sticks',\n",
    "'Then, they sang and danced and played together the rest of the day.',\n",
    "'The third little pig worked hard all day and built his house with bricks',\n",
    "'It was a sturdy house complete with a fine fireplace and chimney',\n",
    "'It looked like it could withstand the strongest winds.',\n",
    "'The next day, a wolf happened to pass by the lane where the three little pigs lived; and he saw the straw house, and he smelled the pig inside',\n",
    "'He thought the pig would make a mighty fine meal and his mouth began to water.',\n",
    "'So he knocked on the door and said:',\n",
    "' Little pig! Little pig!',\n",
    "' Let me in! Let me in!',\n",
    "\"But the little pig saw the wolf's big paws through the keyhole, so he answered back:\",\n",
    "' No! No! No! ',\n",
    "' Not by the hairs on my chinny chin chin!',\n",
    "'Then the wolf showed his teeth and said:',\n",
    "\" Then I'll huff \",\n",
    "\" and I'll puff \",\n",
    "\" and I'll blow your house down.\",\n",
    "'So he huffed and he puffed and he blew the house down! The wolf opened his jaws very wide and bit down as hard as he could, but the first little pig escaped and ran away to hide with the second little pig.',\n",
    "'The wolf continued down the lane and he passed by the second house made of sticks; and he saw the house, and he smelled the pigs inside, and his mouth began to water as he thought about the fine dinner they would make.',\n",
    "'So he knocked on the door and said:',\n",
    "' Little pigs! Little pigs!',\n",
    "' Let me in! Let me in!',\n",
    "\"But the little pigs saw the wolf's pointy ears through the keyhole, so they answered back:\",\n",
    "' No! No! No!',\n",
    "' Not by the hairs on our chinny chin chin!',\n",
    "'So the wolf showed his teeth and said:',\n",
    "\" Then I'll huff \",\n",
    "\" and I'll puff \",\n",
    "\" and I'll blow your house down!\",\n",
    "'So he huffed and he puffed and he blew the house down! The wolf was greedy and he tried to catch both pigs at once, but he was too greedy and got neither! His big jaws clamped down on nothing but air and the two little pigs scrambled away as fast as their little hooves would carry them.',\n",
    "'The wolf chased them down the lane and he almost caught them',\n",
    "'But they made it to the brick house and slammed the door closed before the wolf could catch them',\n",
    "'The three little pigs they were very frightened, they knew the wolf wanted to eat them',\n",
    "'And that was very, very true',\n",
    "\"The wolf hadn't eaten all day and he had worked up a large appetite chasing the pigs around and now he could smell all three of them inside and he knew that the three little pigs would make a lovely feast.\",\n",
    "'So the wolf knocked on the door and said:',\n",
    "' Little pigs! Little pigs!',\n",
    "' Let me in! Let me in!',\n",
    "\"But the little pigs saw the wolf's narrow eyes through the keyhole, so they answered back:\",\n",
    "' No! No! No! ',\n",
    "' Not by the hairs on our chinny chin chin!',\n",
    "'So the wolf showed his teeth and said:',\n",
    "\" Then I'll huff \",\n",
    "\" and I'll puff \",\n",
    "\" and I'll blow your house down.\",\n",
    "'Well! he huffed and he puffed',\n",
    "'He puffed and he huffed',\n",
    "'And he huffed, huffed, and he puffed, puffed; but he could not blow the house down',\n",
    "\"At last, he was so out of breath that he couldn't huff and he couldn't puff anymore\",\n",
    "'So he stopped to rest and thought a bit.',\n",
    "'But this was too much',\n",
    "'The wolf danced about with rage and swore he would come down the chimney and eat up the little pig for his supper',\n",
    "'But while he was climbing on to the roof the little pig made up a blazing fire and put on a big pot full of water to boil',\n",
    "'Then, just as the wolf was coming down the chimney, the little piggy pulled off the lid, and plop! in fell the wolf into the scalding water.',\n",
    "'So the little piggy put on the cover again, boiled the wolf up, and the three little pigs ate him for supper.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNumber  Tag  Description\\n1.\\tCC\\tCoordinating conjunction\\n2.\\tCD\\tCardinal number\\n3.\\tDT\\tDeterminer\\n4.\\tEX\\tExistential there\\n5.\\tFW\\tForeign word\\n6.\\tIN\\tPreposition or subordinating conjunction\\n7.\\tJJ\\tAdjective\\n8.\\tJJR\\tAdjective, comparative\\n9.\\tJJS\\tAdjective, superlative\\n10.\\tLS\\tList item marker\\n11.\\tMD\\tModal\\n12.\\tNN\\tNoun, singular or mass\\n13.\\tNNS\\tNoun, plural\\n14.\\tNNP\\tProper noun, singular\\n15.\\tNNPS\\tProper noun, plural\\n16.\\tPDT\\tPredeterminer\\n17.\\tPOS\\tPossessive ending\\n18.\\tPRP\\tPersonal pronoun\\n19.\\tPRP$\\tPossessive pronoun\\n20.\\tRB\\tAdverb\\n21.\\tRBR\\tAdverb, comparative\\n22.\\tRBS\\tAdverb, superlative\\n23.\\tRP\\tParticle\\n24.\\tSYM\\tSymbol\\n25.\\tTO\\tto\\n26.\\tUH\\tInterjection\\n27.\\tVB\\tVerb, base form\\n28.\\tVBD\\tVerb, past tense\\n29.\\tVBG\\tVerb, gerund or present participle\\n30.\\tVBN\\tVerb, past participle\\n31.\\tVBP\\tVerb, non-3rd person singular present\\n32.\\tVBZ\\tVerb, 3rd person singular present\\n33.\\tWDT\\tWh-determiner\\n34.\\tWP\\tWh-pronoun\\n35.\\tWP$\\tPossessive wh-pronoun\\n36.\\tWRB\\tWh-adverb\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Number  Tag  Description\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A POS tag means Part-of-Speech tag. It’s a label that tells you the grammatical role of a word in a sentence**\n",
    "\n",
    "For example:\n",
    "\n",
    "Noun (NN) – person, place, thing (e.g., dog, car, idea)\n",
    "\n",
    "Verb (VB) – action or state (e.g., run, is, think)\n",
    "\n",
    "Adjective (JJ) – describes a noun (e.g., happy, big, red)\n",
    "\n",
    "Adverb (RB) – describes a verb/adjective (e.g., quickly, very)\n",
    "\n",
    "Pronoun (PRP) – replaces a noun (e.g., he, she, it)\n",
    "\n",
    "Determiner (DT) – introduces a noun (e.g., the, a, this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pos_tagged_text=[\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text))]\n\u001b[32m      2\u001b[39m pos_tagged_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_text=[nltk.pos_tag(text[i].strip().split(' ')) for i in range(len(text))]\n",
    "pos_tagged_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # иногда требуется для мультиязычной поддержки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(t):\n",
    "    try:\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "        \"P\": wordnet.NOUN,#\n",
    "        \"C\": wordnet.NOUN,\n",
    "        \"D\": wordnet.NOUN,\n",
    "        \"E\": wordnet.NOUN,\n",
    "        \"I\": wordnet.NOUN,\n",
    "        \"L\": wordnet.NOUN,\n",
    "        \"M\": wordnet.NOUN,\n",
    "        \"T\": wordnet.NOUN,\n",
    "        \"U\": wordnet.NOUN,\n",
    "        \"W\": wordnet.NOUN,\n",
    "        \"F\": wordnet.NOUN,\n",
    "        \"S\": wordnet.NOUN}\n",
    "        return tag_dict[t[0]]\n",
    "    except:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos_tag('reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mI\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreading\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43man\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minteresting\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbook\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.pos_tag(['I','am','reading', 'an', 'interesting', 'book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m      2\u001b[39m lemmatized_text = [\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join([lemmatizer.lemmatize(tg[\u001b[32m0\u001b[39m], pos=get_pos_tag(tg[\u001b[32m1\u001b[39m]))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang, loc)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.save_dir = path_join(\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.TRAINED_TAGGER_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.TAGGER_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang, loc)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m, loc=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_param\u001b[39m(json_file):\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_join(loc, json_file)) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = [' '.join([lemmatizer.lemmatize(tg[0], pos=get_pos_tag(tg[1]))\n",
    "for tg in nltk.pos_tag(s.strip().split(' '))])\n",
    "for s in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them', 'So when they were old enough, she sent them out into the world to seek their fortunes.', 'The first little pig was very lazy', \"He didn't want to work at all and he built his house out of straw\", 'The second little pig worked a little bit harder but he was somewhat lazy too and he built his house out of sticks']\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(text[:\u001b[32m5\u001b[39m])\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatized_text\u001b[49m[:\u001b[32m5\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'lemmatized_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(text[:5])\n",
    "print('-------------------------------')\n",
    "print(lemmatized_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['onc',\n",
       "  'upon',\n",
       "  'a',\n",
       "  'time',\n",
       "  'there',\n",
       "  'wa',\n",
       "  'an',\n",
       "  'old',\n",
       "  'mother',\n",
       "  'pig',\n",
       "  'who',\n",
       "  'had',\n",
       "  'three',\n",
       "  'littl',\n",
       "  'pig',\n",
       "  'and',\n",
       "  'not',\n",
       "  'enough',\n",
       "  'food',\n",
       "  'to',\n",
       "  'feed',\n",
       "  'them'],\n",
       " ['so',\n",
       "  'when',\n",
       "  'they',\n",
       "  'were',\n",
       "  'old',\n",
       "  'enough,',\n",
       "  'she',\n",
       "  'sent',\n",
       "  'them',\n",
       "  'out',\n",
       "  'into',\n",
       "  'the',\n",
       "  'world',\n",
       "  'to',\n",
       "  'seek',\n",
       "  'their',\n",
       "  'fortunes.'],\n",
       " ['the', 'first', 'littl', 'pig', 'wa', 'veri', 'lazi'],\n",
       " ['he',\n",
       "  \"didn't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'work',\n",
       "  'at',\n",
       "  'all',\n",
       "  'and',\n",
       "  'he',\n",
       "  'built',\n",
       "  'hi',\n",
       "  'hous',\n",
       "  'out',\n",
       "  'of',\n",
       "  'straw'],\n",
       " ['the',\n",
       "  'second',\n",
       "  'littl',\n",
       "  'pig',\n",
       "  'work',\n",
       "  'a',\n",
       "  'littl',\n",
       "  'bit',\n",
       "  'harder',\n",
       "  'but',\n",
       "  'he',\n",
       "  'wa',\n",
       "  'somewhat',\n",
       "  'lazi',\n",
       "  'too',\n",
       "  'and',\n",
       "  'he',\n",
       "  'built',\n",
       "  'hi',\n",
       "  'hous',\n",
       "  'out',\n",
       "  'of',\n",
       "  'stick']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_text=[[ps.stem(w) for w in s.strip().split()] for s in text]\n",
    "stemmed_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your own algorithms for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_stem(list_words):\n",
    "    \"\"\"\n",
    "    Простой алгоритм стемминга, удаляющий распространённые суффиксы\n",
    "    \"\"\"\n",
    "    # Список суффиксов в порядке приоритета (от более длинных к коротким)\n",
    "    suffixes = ['iness', 'ness', 'ing', 'ied', 'ies', 'ed', 'es', 's', 'ly', 'er', 'est', 'ful', 'less', 'ment', 'able', 'ible']\n",
    "    \n",
    "    splitted = list_words.split(' ')\n",
    "    stemmed_words = []\n",
    "    \n",
    "    for word in splitted:\n",
    "        # Приводим к нижнему регистру для обработки\n",
    "        lower_word = word.lower()\n",
    "        stemmed = lower_word\n",
    "        \n",
    "        # Пробуем удалить суффиксы\n",
    "        for suffix in suffixes:\n",
    "            if lower_word.endswith(suffix) and len(lower_word) > len(suffix) + 2:\n",
    "                # Оставляем минимум 3 символа в основе слова\n",
    "                stemmed = lower_word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        stemmed_words.append(stemmed)\n",
    "    \n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальное предложение:\n",
      "Once upon a time there was an old mother pig who had three little pigs\n",
      "\n",
      "Результат нашего стемминга:\n",
      "['once', 'upon', 'a', 'time', 'there', 'was', 'an', 'old', 'moth', 'pig', 'who', 'had', 'three', 'little', 'pig']\n",
      "\n",
      "Результат PorterStemmer:\n",
      "['onc', 'upon', 'a', 'time', 'there', 'wa', 'an', 'old', 'mother', 'pig', 'who', 'had', 'three', 'littl', 'pig']\n"
     ]
    }
   ],
   "source": [
    "# Тестирование нашей функции стемминга\n",
    "test_sentence = \"Once upon a time there was an old mother pig who had three little pigs\"\n",
    "print(\"Оригинальное предложение:\")\n",
    "print(test_sentence)\n",
    "print(\"\\nРезультат нашего стемминга:\")\n",
    "print(own_stem(test_sentence))\n",
    "\n",
    "# Сравним с PorterStemmer\n",
    "print(\"\\nРезультат PorterStemmer:\")\n",
    "ps = PorterStemmer()\n",
    "print([ps.stem(w) for w in test_sentence.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст:\n",
      "['Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them', 'So when they were old enough, she sent them out into the world to seek their fortunes.', 'The first little pig was very lazy']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Наш стемминг:\n",
      "[['once', 'upon', 'a', 'time', 'there', 'was', 'an', 'old', 'moth', 'pig', 'who', 'had', 'three', 'little', 'pig', 'and', 'not', 'enough', 'food', 'to', 'feed', 'them'], ['so', 'when', 'they', 'were', 'old', 'enough,', 'she', 'sent', 'them', 'out', 'into', 'the', 'world', 'to', 'seek', 'their', 'fortunes.'], ['the', 'first', 'little', 'pig', 'was', 'very', 'lazy']]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "PorterStemmer:\n",
      "[['onc', 'upon', 'a', 'time', 'there', 'wa', 'an', 'old', 'mother', 'pig', 'who', 'had', 'three', 'littl', 'pig', 'and', 'not', 'enough', 'food', 'to', 'feed', 'them'], ['so', 'when', 'they', 'were', 'old', 'enough,', 'she', 'sent', 'them', 'out', 'into', 'the', 'world', 'to', 'seek', 'their', 'fortunes.'], ['the', 'first', 'littl', 'pig', 'wa', 'veri', 'lazi']]\n"
     ]
    }
   ],
   "source": [
    "# Применяем наш алгоритм стемминга ко всему тексту\n",
    "own_stemmed_text = [own_stem(s) for s in text]\n",
    "\n",
    "# Сравнение первых 3 предложений\n",
    "print(\"Оригинальный текст:\")\n",
    "print(text[:3])\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Наш стемминг:\")\n",
    "print(own_stemmed_text[:3])\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"PorterStemmer:\")\n",
    "print(stemmed_text[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение алгоритмов стемминга\n",
    "\n",
    "**Наш алгоритм `own_stem`:**\n",
    "- Использует простое правило удаления суффиксов\n",
    "- Обрабатывает распространённые английские суффиксы: -ing, -ed, -es, -s, -ly, -er, -est, -ful, -less, -ment, -able, -ible\n",
    "- Сохраняет минимум 3 символа в основе слова\n",
    "- Применяет только один суффикс на слово (первый подходящий)\n",
    "\n",
    "**PorterStemmer:**\n",
    "- Более сложный алгоритм с несколькими этапами\n",
    "- Учитывает фонетические правила английского языка\n",
    "- Может давать более агрессивное сокращение слов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры обработки слов:\n",
      "\n",
      "Слово           Наш алгоритм    PorterStemmer  \n",
      "--------------------------------------------------\n",
      "running         runn            run            \n",
      "runner          runn            runner         \n",
      "easily          easi            easili         \n",
      "happiness       happ            happi          \n",
      "worked          work            work           \n",
      "houses          hous            hous           \n",
      "building        build           build          \n",
      "beautiful       beauti          beauti         \n"
     ]
    }
   ],
   "source": [
    "# Статистика и примеры различий\n",
    "print(\"Примеры обработки слов:\\n\")\n",
    "test_words = [\"running\", \"runner\", \"easily\", \"happiness\", \"worked\", \"houses\", \"building\", \"beautiful\"]\n",
    "\n",
    "print(f\"{'Слово':<15} {'Наш алгоритм':<15} {'PorterStemmer':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for word in test_words:\n",
    "    our_result = own_stem(word)[0]\n",
    "    porter_result = ps.stem(word)\n",
    "    print(f\"{word:<15} {our_result:<15} {porter_result:<15}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
